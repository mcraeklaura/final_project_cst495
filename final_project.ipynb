{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "For our final project we are using the CIFAR10 dataset. The dataset consists of 60000 RGB images that are divided into ten different classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck. Each image is 32x32 pixels and 50000 of them are dedicated as training images and 10,000 of them are for testing. Each class has 5,000 images. The labels are numerical numbers 0-9 that represent the different classes.\n",
    "\n",
    "# Plan\n",
    "Our goal is to create a Convolutional Neural Network that will achieve the highest accuracy on the test set. Some of our concerns are if there are too many images for the computing power we have currently. There are several paid options that we can utilize, but computing time is a concern with us.\n",
    "\n",
    "First, we plan to try and put our data into the basic MNIST CNN that we have been using in class. From there, we will make necessary changes. We would like to look into data augmentation and see if creating new image objects from the images we already have would help training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Status\n",
    "We were able to get an initial model running, but the accuracy was low and variable, ranging from 20-30%. However, this is not too bad because if we were to make predictions at random, we would expect to get 10% accuracy. We were using the generic CNN model we were using for the MNIST data, which could be why it is not doing as well, because the MNIST data did not take color and filters into account. We used some of the starter code from the in class lab to try and create a model that would make more accurate predictions.\n",
    "\n",
    "When first training, there was a point in which the training accuracy dropped from 1.0 to .10. Using small batch sizes, such as 64, resulted in strange behavior.  Increasing the batch size to around 1000 significantly increased the training and testing accuracy. \n",
    "\n",
    "It may be possible that the huge decrease in accuracy occurs because the net was fed sets of images that happened to be from the same class, and then when the next batch was comprised of different classes, the net performed poorly. Therefore, in combining dropout with larger batch sizes, we think that we can remove this issue. Preliminary testing suggests that dropout does not help, though we have not experimented with it enough to definitively say.\n",
    "\n",
    "## Further Work\n",
    "For the rest of our project we plan to try and dive deep into different issues with this data set and figure out what exactly the model needs to be successful. Moving different layers and pooling around did not seem to help, so we are thinking of adding more layers and pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from __future__ import division, print_function, unicode_literals # is this needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"cifar-10-batches-py/data_batch_\"\n",
    "batches = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    batches.append(unpickle(data_path + str(i)))\n",
    "    \n",
    "test_path = \"cifar-10-batches-py/test_batch\"\n",
    "test = unpickle(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = batches[0][b'data']\n",
    "y_train = batches[0][b'labels']\n",
    "\n",
    "for i in range(1,5):\n",
    "    X_train = np.concatenate((X_train, batches[i][b'data']))\n",
    "    y_train = np.concatenate((y_train, batches[i][b'labels']))\n",
    "    \n",
    "test_data = test[b'data']\n",
    "test_labels = test[b'labels']\n",
    "\n",
    "X_test_full = test_data\n",
    "y_test_full = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = np.array(list(range(X_test_full.shape[0])))\n",
    "indices = np.random.choice(numbers, X_test_full.shape[0]//2, replace=False)\n",
    "\n",
    "other_indices = [x not in indices for x in numbers]\n",
    "\n",
    "X_test = X_test_full[indices]\n",
    "y_test = y_test_full.reshape(-1,)[indices] \n",
    "\n",
    "X_val = X_test_full[other_indices]\n",
    "y_val = y_test_full.reshape(-1,)[other_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "n_inputs = height * width * channels\n",
    "\n",
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "n_fc1 = 64\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 8 * 8])\n",
    "\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, kernel_initializer=he_init, activation=tf.nn.relu, name=\"fc1\")\n",
    "\n",
    "#drop = tf.layers.dropout(fc1, .2, name=\"drop\")\n",
    "    \n",
    "#fc2 = tf.layers.dense(drop, n_fc1*2, kernel_initializer=he_init, activation=tf.nn.relu, name=\"fc2\")\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001,\n",
    "                                        beta1=0.9,\n",
    "                                        beta2=0.999,\n",
    "                                        epsilon=1e-06,)\n",
    "    \n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_size):\n",
    "    np.random.seed(epoch * batch_size) \n",
    "    indices = np.random.randint(X_train.shape[0], size=batch_size) \n",
    "    X_batch = X_train[indices]\n",
    "    y_batch = y_train[indices] \n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.1085\n",
      "0 Test accuracy: 0.1068\n",
      "1 Train accuracy: 0.114\n",
      "1 Test accuracy: 0.1142\n",
      "2 Train accuracy: 0.14725\n",
      "2 Test accuracy: 0.1344\n",
      "3 Train accuracy: 0.19475\n",
      "3 Test accuracy: 0.1816\n",
      "4 Train accuracy: 0.26125\n",
      "4 Test accuracy: 0.242\n",
      "5 Train accuracy: 0.3055\n",
      "5 Test accuracy: 0.2878\n",
      "6 Train accuracy: 0.365\n",
      "6 Test accuracy: 0.3302\n",
      "7 Train accuracy: 0.4015\n",
      "7 Test accuracy: 0.3522\n",
      "8 Train accuracy: 0.418\n",
      "8 Test accuracy: 0.373\n",
      "9 Train accuracy: 0.44725\n",
      "9 Test accuracy: 0.378\n",
      "10 Train accuracy: 0.474\n",
      "10 Test accuracy: 0.3908\n",
      "11 Train accuracy: 0.485\n",
      "11 Test accuracy: 0.3984\n",
      "12 Train accuracy: 0.5\n",
      "12 Test accuracy: 0.41\n",
      "13 Train accuracy: 0.4635\n",
      "13 Test accuracy: 0.4056\n",
      "14 Train accuracy: 0.51175\n",
      "14 Test accuracy: 0.4288\n",
      "15 Train accuracy: 0.52325\n",
      "15 Test accuracy: 0.4274\n",
      "16 Train accuracy: 0.5485\n",
      "16 Test accuracy: 0.435\n",
      "17 Train accuracy: 0.53425\n",
      "17 Test accuracy: 0.4372\n",
      "18 Train accuracy: 0.569\n",
      "18 Test accuracy: 0.4444\n",
      "19 Train accuracy: 0.5625\n",
      "19 Test accuracy: 0.4418\n",
      "20 Train accuracy: 0.55325\n",
      "20 Test accuracy: 0.4464\n",
      "21 Train accuracy: 0.57925\n",
      "21 Test accuracy: 0.4492\n",
      "22 Train accuracy: 0.5975\n",
      "22 Test accuracy: 0.4598\n",
      "23 Train accuracy: 0.5985\n",
      "23 Test accuracy: 0.4638\n",
      "24 Train accuracy: 0.5935\n",
      "24 Test accuracy: 0.4642\n",
      "25 Train accuracy: 0.59975\n",
      "25 Test accuracy: 0.4708\n",
      "26 Train accuracy: 0.59875\n",
      "26 Test accuracy: 0.455\n",
      "27 Train accuracy: 0.6145\n",
      "27 Test accuracy: 0.4734\n",
      "28 Train accuracy: 0.6065\n",
      "28 Test accuracy: 0.482\n",
      "29 Train accuracy: 0.622\n",
      "29 Test accuracy: 0.4826\n",
      "30 Train accuracy: 0.62425\n",
      "30 Test accuracy: 0.4794\n",
      "31 Train accuracy: 0.643\n",
      "31 Test accuracy: 0.4872\n",
      "32 Train accuracy: 0.65725\n",
      "32 Test accuracy: 0.4822\n",
      "33 Train accuracy: 0.647\n",
      "33 Test accuracy: 0.4824\n",
      "34 Train accuracy: 0.65125\n",
      "34 Test accuracy: 0.4862\n",
      "35 Train accuracy: 0.64975\n",
      "35 Test accuracy: 0.4766\n",
      "36 Train accuracy: 0.68\n",
      "36 Test accuracy: 0.4876\n",
      "37 Train accuracy: 0.67675\n",
      "37 Test accuracy: 0.4926\n",
      "38 Train accuracy: 0.65975\n",
      "38 Test accuracy: 0.4892\n",
      "39 Train accuracy: 0.652\n",
      "39 Test accuracy: 0.487\n",
      "40 Train accuracy: 0.678\n",
      "40 Test accuracy: 0.4834\n",
      "41 Train accuracy: 0.669\n",
      "41 Test accuracy: 0.497\n",
      "42 Train accuracy: 0.67475\n",
      "42 Test accuracy: 0.4908\n",
      "43 Train accuracy: 0.68775\n",
      "43 Test accuracy: 0.493\n",
      "44 Train accuracy: 0.6915\n",
      "44 Test accuracy: 0.4956\n",
      "45 Train accuracy: 0.6955\n",
      "45 Test accuracy: 0.5\n",
      "46 Train accuracy: 0.68125\n",
      "46 Test accuracy: 0.4924\n",
      "47 Train accuracy: 0.69975\n",
      "47 Test accuracy: 0.5026\n",
      "48 Train accuracy: 0.69225\n",
      "48 Test accuracy: 0.4976\n",
      "49 Train accuracy: 0.71625\n",
      "49 Test accuracy: 0.4996\n",
      "Other Test accuracy: 0.5128\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 4000\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(X_train.shape[0] // batch_size):\n",
    "            \n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        print(epoch, \"Train accuracy:\", acc_train)\n",
    "        train_acc.append(acc_train)\n",
    "        \n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Test accuracy:\", acc_test)\n",
    "        test_acc.append(test_acc)\n",
    "        \n",
    "    acc_other_test = accuracy.eval(feed_dict={X:X_val, y:y_val})\n",
    "    print(\"Other Test accuracy:\", acc_other_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
